\section{Lecture 2: Markov Decision Processes}

\noindent Markov decision processes formally describe an environment for reinforcement
learning.

\subsection{Markov Process}

A Markov process or Markov Chain is a memoryless random process represented by
a tuple $\langle\mathcal{S}, \mathcal{P}\rangle$ where $\mathcal{S}$ is a
finite set of states and $\mathcal{P}$ is the state transition probability. The
state transition matrix $\mathcal{P}$ defines the transition probability from
all states $s$ to all successor states $s'$

\[
    \mathcal{P}_{ss'} = \mathbb{P}[S_{t+1} = s' | S_t = s]
\]

\[
    \mathcal{P} = \begin{bmatrix}
        \mathcal{P}_{11} & \cdots & \mathcal{P}_{1n} \\
        \vdots           & \ddots & \vdots           \\
        \mathcal{P}_{n1} & \cdots & \mathcal{P}_{nn}
    \end{bmatrix}
\]

\subsection{Markov Reward Process}

A Markov reward process is a Markov process with reward function $\mathcal{R}$
and discount factor $\gamma$. It is represented by a tuple $\langle\mathcal{S},
    \mathcal{P}, \mathcal{R}, \gamma\rangle$ where $\mathcal{S}$ is a finite set of
states, $\mathcal{P}$ is the state transition probability, $\mathcal{R}$ is the
reward function and $\gamma$ is the discount factor.

\begin{gather*}
    \mathcal{R}_s = \mathbb{E}[R_{t+1} | S_t = s] \\
    \gamma \in [0, 1]
\end{gather*}

\noindent The return $G_t$ is the total discounted reward from time step $t$.

\[
    G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
\]

\noindent The value function $v(s)$ is the expected return starting from state $s$.
According to Bellman equation, the value function can be decomposed into
immediate reward and the discounted value of the successor state.

\begin{align*}
    v(s)       & = \mathbb{E}[G_t | S_t = s]                                                  \\
               & = \mathbb{E}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots | S_t = s] \\
               & = \mathbb{E}[R_{t+1} + \gamma G_{t+1} | S_t = s]                             \\
               & = \mathbb{E}[R_{t+1} + \gamma v(S_{t+1}) | S_t = s]                          \\
               & = \mathcal{R}_s + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'} v(s')   \\
    \mathbf{v} & = \mathcal{R} + \gamma \mathcal{P} \mathbf{v} \text{  (matrices form)}       \\
    \mathbf{v} & = (\mathbf{I} - \gamma \mathcal{P})^{-1} \mathcal{R} \text{  (solution)}     \\
\end{align*}

\[
    \mathbf{v} =
    \begin{bmatrix}
        v(1)   \\
        \vdots \\
        v(n)
    \end{bmatrix} =
    \begin{bmatrix}
        \mathcal{R}_1 \\
        \vdots        \\
        \mathcal{R}_n
    \end{bmatrix}
    +
    \gamma
    \begin{bmatrix}
        \mathcal{P}_{11} & \cdots & \mathcal{P}_{1n} \\
        \vdots           & \ddots & \vdots           \\
        \mathcal{P}_{n1} & \cdots & \mathcal{P}_{nn}
    \end{bmatrix}
    \begin{bmatrix}
        v(1)   \\
        \vdots \\
        v(n)
    \end{bmatrix}
\]

\subsection{Markov Decision Process}

A Markov decision process is a Markov reward process with decisions. All states
are Markov in this environment. It is represented by a tuple
$\langle\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma\rangle$
where $\mathcal{S}$ is a finite set of states, $\mathcal{A}$ is a finite set of
actions, $\mathcal{P}$ is the state transition probability, $\mathcal{R}$ is
the reward function and $\gamma$ is the discount factor.

\[
    \mathcal{P}_{ss'}^a = \mathbb{P}[S_{t+1} = s' | S_t = s, A_t = a]
\]

\[
    \mathcal{R}_s^a = \mathbb{E}[R_{t+1} | S_t = s, A_t = a]
\]

\noindent A policy $\pi$ is a distribution over actions given states.

\[
    \pi(a|s) = \mathbb{P}[A_t = a | S_t = s]
\]

\noindent The state-value function $v_{\pi}(s)$ is the expected return starting from state
$s$ and following policy $\pi$.

\begin{align*}
    v_{\pi}(s) & = \mathbb{E}_{\pi}[G_t | S_t = s]                                                  \\
               & = \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1} | S_t = s]                             \\
               & = \mathbb{E}_{\pi}[R_{t+1} + \gamma \mathbb{E}_{\pi}[G_{t+1} | S_{t+1}] | S_t = s] \\
               & = \mathbb{E}_{\pi}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s]                    \\
\end{align*}

\noindent The action-value function $q_{\pi}(s, a)$ is the expected return starting from state
$s$, taking action $a$ and then following policy $\pi$.

\begin{align*}
    q_{\pi}(s, a) & = \mathbb{E}_{\pi}[G_t | S_t = s, A_t = a]                                                          \\
                  & = \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1} | S_t = s, A_t = a]                                     \\
                  & = \mathbb{E}_{\pi}[R_{t+1} + \gamma \mathbb{E}_\pi [G_{t+1} | S_{t+1}, A_{t+1}] | S_t = s, A_t = a] \\
                  & = \mathbb{E}_{\pi}[R_{t+1} + \gamma q_{\pi}(S_{t+1}, A_{t+1}) | S_t = s, A_t = a]                   \\
\end{align*}

\noindent The Bellman expectation equations for state-value function and action-value function are:

\begin{align*}
    v_{\pi}(s)       & = \sum_{a \in \mathcal{A}} \pi(a|s) q_\pi(s,a)                                                                                        \\
    q_{\pi}(s, a)    & = \mathcal{R}_s^a + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a v_{\pi}(s')                                                  \\
    \\
    v_{\pi}(s)       & = \sum_{a \in \mathcal{A}} \pi(a|s) \left( \mathcal{R}_s^a + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a v_{\pi}(s') \right) \\
    \mathbf{v}_{\pi} & = \mathcal{R}^{\pi} + \gamma \mathcal{P}^{\pi} \mathbf{v}_{\pi} \text{  (matrices form)}                                              \\
    \mathbf{v}_{\pi} & = (\mathbf{I} - \gamma \mathcal{P}^{\pi})^{-1} \mathcal{R}^{\pi} \text{  (solution)}                                                  \\
    \\
    q_{\pi}(s, a)    & = \mathcal{R}_s^a + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a \sum_{a' \in \mathcal{A}} \pi(a'|s') q_{\pi}(s', a')         \\
\end{align*}

\noindent An optimal state value function $v_*(s)$ is the maximum value function over all
policies. The optimal action value function $q_*(s, a)$ is the maximum action
value function over all policies.

\begin{gather*}
    v_*(s) = \max_{\pi} v_{\pi}(s) \\
    q_*(s, a) = \max_{\pi} q_{\pi}(s, a)
\end{gather*}

\noindent Bellman optimality equation for state-value function and action-value function
are:

\begin{align*}
    v_*(s)    & = \max_{a \in \mathcal{A}} q_*(s, a)                                                                                     \\
    q_*(s, a) & = \mathcal{R}_s^a + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a v_*(s')                                         \\
    \\
    v_*(s)    & = \max_{a \in \mathcal{A}} \left( \mathcal{R}_s^a + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a v_*(s') \right) \\
    q_*(s, a) & = \mathcal{R}_s^a + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a \max_{a' \in \mathcal{A}} q_*(s', a')           \\
\end{align*}