\section{Model-Free Control}

Optimize the value function $v_*$ and policy $\pi_*$ of an unknown Markov
Decision Process $\langle \mathcal{S}, \mathcal{A}, \gamma, \pi \rangle$, i.e.
no knowledge of transition probability $\mathcal{P}_{ss\prime}^a$ and reward
$\mathcal{R}_s^a$, but the agent do know the actual reward and the actual state
that they end up in.\\

\noindent \textbf{On-policy learning}: Learn on the job, by learning about policy $\pi$ from
experience sampled from $\pi$. \\

\noindent \textbf{Off-policy learning}: Learn over someone's shoulder, by learning about policy
$\pi$ from experience sampled from $\mu$.

\subsection{On-Policy Monte-Carlo Control}

Policy evaluation is done using $Q(s, a)$ since it is model free (i.e. does not
depend on $\mathcal{P}$ and $\mathcal{R}$). \\

\noindent $\epsilon$-Greedy Exploration:

\begin{equation*}
    \pi(a|s) = \begin{cases}
        \epsilon / m + 1 - \epsilon & \text{if } a^* = \argmax_{a \in \mathcal{A}} Q(s, a) \\
        \epsilon / m                & \text{otherwise}
    \end{cases}
\end{equation*}

\noindent It can be proof that for any $\epsilon$-greedy policy $\pi$, the
$\epsilon$-greedy policy $\pi^\prime$ with respect to $Q_\pi$ is an
improvement, i.e. $v_{\pi\prime}(s) \ge v_\pi(s)$. \\

\noindent Greedy in the Limit with Infinite Exploration (GLIE) is a set of two conditions
that, if met, ensure that an algorithm will eventually find the optimal policy.
The conditions are:

\begin{itemize}
    \item \textbf{Infinite Exploration}: All state-action pairs are explored infinitely many times,
          $\lim_{k \to \infty} N_k(s, a) = \infty$ where $N_k(s, a)$ is the number of times action
          $a$ has been taken in state $s$ up to time step $k$.
    \item \textbf{Greedy in the Limit}: The policy converges to a greedy policy,
          $\lim_{k \to \infty} \pi_k(a|s) = 1$ if $a = \argmax_{a \in \mathcal{A}} Q_k(s, a)$ and
          0 otherwise.
\end{itemize}

\noindent The GLIE Monte-Carlo control algorithm is as follows:

\begin{algorithm}[H]
    \caption{GLIE Monte-Carlo control}
    \begin{algorithmic}
        \State Sample $k$th episode using $\pi$: $\{S_1, A_1, R_2, \dots, S_T\} \sim \pi$
        \For{each state $S_t$ and action $A_t$ in the episode}
        \State $N(S_t, A_t) \leftarrow N(S_t, A_t) + 1$
        \State $Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \frac{1}{N(S_t, A_t)} (G_t - Q(S_t, A_t))$
        \EndFor
        \State Improve policy based on the new action-value function
        \State $\epsilon \leftarrow 1/k$
        \State $\pi \leftarrow \epsilon\text{-greedy}(Q)$
    \end{algorithmic}
\end{algorithm}

\noindent GLIE Monte-Carlo control converges to the optimal action-value function, $Q(s,
    a) \rightarrow q_*(s, a)$

\subsection{On-Policy Temporal-Difference Control}

\textbf{Sarsa} algorithm:

\begin{itemize}
    \item Policy evaluation: $Q(S, A) \leftarrow Q(S, A) + \alpha (R + \gamma Q(S', A') -
              Q(S, A))$
    \item Policy improvement: $\epsilon$-greedy policy improvement is used
\end{itemize}

\noindent Sarsa algorithm for on-policy control is as follows:

\begin{algorithm}[H]
    \caption{Sarsa}
    \begin{algorithmic}
        \State Initialize $Q(s, a), \forall s \in \mathcal{S}, a \in \mathcal{A}(s)$, arbitrarily, and $Q(\text{terminal-state}, \cdot) = 0$
        \Repeat{ for each episode}
        \State Initialize $S$
        \State Choose $A$ from $S$ using policy derived from $Q$ (e.g., $\epsilon$-greedy)
        \Repeat{ for each step of episode}
        \State Take action $A$, observe $R, S'$
        \State Choose $A'$ from $S'$ using policy derived from $Q$ (e.g., $\epsilon$-greedy)
        \State $Q(S, A) \leftarrow Q(S, A) + \alpha[R + \gamma Q(S', A') - Q(S, A)]$
        \State $S \leftarrow S'; A \leftarrow A';$
        \Until{$S$ is terminal}
        \Until{end}
    \end{algorithmic}
\end{algorithm}

\noindent \textbf{n-Step Sarsa}: Instead of updating $Q$ based on a single step, we can update it using n steps:

\[
    q_t^{(n)} = R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1} R_{t+n} + \gamma^n Q(S_{t+n}, A_{t+n})
\]

\noindent The n-step Sarsa policy evaluation updates is as follows:

\[
    Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left( q_t^{(n)} - Q(S_t, A_t) \right)
\]

\noindent \textbf{Forward View Sarsa($\lambda$)}: Similar to TD($\lambda$), we can define the
n-step return using weighted sum of n-step returns:

\begin{gather*}
    q_t^{\lambda} = (1 - \lambda) \sum_{n=1}^{\infty} \lambda^{n-1} q_t^{(n)} \\
    Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left( q_t^{\lambda} - Q(S_t, A_t) \right)
\end{gather*}

\noindent \textbf{Backward view Sarsa($\lambda$)} uses eligibility traces to update the
action-value function:

\begin{align*}
    E_0(s, a) & = 0                                                  \\
    E_t(s, a) & = \gamma \lambda E_{t-1}(s, a) + 1(S_t = s, A_t = a) \\
    \delta_t  & = R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \\
    Q(s, a)   & \leftarrow Q(s, a) + \alpha \delta_t E_t(s, a)
\end{align*}\

\noindent Sarsa($\lambda$) algorithm is as follows:

\begin{algorithm}[H]
    \caption{Sarsa($\lambda$)}
    \begin{algorithmic}
        \State Initialize $Q(s, a)$ arbitrarily, for all $s \in \mathcal{S}, a \in \mathcal{A}(s)$
        \Repeat{ for each episode}
        \State $E(s, a) = 0$, for all $s \in \mathcal{S}, a \in \mathcal{A}(s)$
        \State Initialize $S, A$
        \Repeat{ for each step of episode}
        \State Take action $A$, observe $R, S'$
        \State Choose $A'$ from $S'$ using policy derived from $Q$ (e.g., $\epsilon$-greedy)
        \State $\delta \leftarrow R + \gamma Q(S', A') - Q(S, A)$
        \State $E(S, A) \leftarrow E(S, A) + 1$
        \For{all $s \in \mathcal{S}, a \in \mathcal{A}(s)$}
        \State $Q(s, a) \leftarrow Q(s, a) + \alpha \delta E(s, a)$
        \State $E(s, a) \leftarrow \gamma \lambda E(s, a)$
        \EndFor
        \State $S \leftarrow S'; A \leftarrow A'$
        \Until{$S$ is terminal}
        \Until{end}
    \end{algorithmic}
\end{algorithm}

\subsection{Off-Policy Learning}

\noindent \textbf{Off-policy learning}: Evaluate target policy $\pi$ while following behavior
policy $\mu$ to compute $v_\pi(s)$ or $q_\pi(s, a)$. \\

\noindent \textbf{Off-policy Monte-Carlo using Importance Sampling}:

\begin{gather*}
    G_t^{\pi / \mu} = \frac{\pi(A_t|S_t)}{\mu(A_t|S_t)} \frac{\pi(A_{t+1}|S_{t+1})}{\mu(A_{t+1}|S_{t+1})} \cdots \frac{\pi(A_{T}|S_{T})}{\mu(A_{T}|S_{T})} G_t \\
    V(S_t) \leftarrow V(S_t) + \alpha \left( G_t^{\pi / \mu} - V(S_t) \right)
\end{gather*}

\noindent \textbf{Off-policy TD using Imporantance Sampling}:

\begin{gather*}
    V(S_t) \leftarrow V(S_t) + \alpha \left( \frac{\pi(A_t|S_t)}{\mu(A_t|S_t)} (R_{t+1} + \gamma V(S_t+1)) - V(S_t) \right)
\end{gather*}

\noindent \textbf{Q-learning}:

\begin{align*}
    \pi(S_{t+1})                    & = \argmax_{a'} Q(S_{t+1}, a')                                                             \\
    R_{t+1} + \gamma Q(S_{t+1}, A') & = R_{t+1} + \gamma Q(S_{t+1}, \argmax_{a'} Q(S_{t+1}, a'))                                \\
                                    & = R_{t+1} + \gamma \max_{a'} Q(S_{t+1}, a')                                               \\
    Q(S_t, A_t)                     & \leftarrow Q(S_t, A_t) + \alpha (R_{t+1} + \gamma Q(S_{t+1}, A') - Q(S_t, A_t))           \\
    Q(S_t, A_t)                     & \leftarrow Q(S_t, A_t) + \alpha (R_{t+1} + \gamma \max_{a'} Q(S_{t+1}, a') - Q(S_t, A_t))
\end{align*}

\noindent Q-learning algorithm is as follows:

\begin{algorithm}[H]
    \caption{Q-learning}
    \begin{algorithmic}
        \State Initialize $Q(s, a), \forall s \in \mathcal{S}, a \in \mathcal{A}(s)$, arbitrarily, and $Q(\text{terminal-state}, \cdot) = 0$
        \Repeat{ for each episode}
        \State Initialize $S$
        \Repeat
        \State Choose $A$ from $S$ using policy derived from $Q$ (e.g., $\epsilon$-greedy)
        \State Take action $A$, observe $R, S'$
        \State $Q(S, A) \leftarrow Q(S, A) + \alpha[R + \gamma \max_{a'} Q(S', a') - Q(S, A)]$
        \State $S \leftarrow S'$
        \Until{$S$ is terminal}
        \Until{end}
    \end{algorithmic}
\end{algorithm}
