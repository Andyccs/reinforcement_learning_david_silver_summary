\section{Lecture 5: Model-Free Control}

Optimize the value function $v_*$ and policy $\pi_*$ of an unknown Markov
Decision Process $\langle \mathcal{S}, \mathcal{A}, \gamma, \pi \rangle$, i.e.
no knowledge of transition probability $\mathcal{P}_{ss\prime}^a$ and reward
$\mathcal{R}_s^a$, but the agent do know the actual reward and the actual state
that they end up in.\\

\noindent \textbf{On-policy learning}: Learn on the job, by learning about policy $\pi$ from
experience sampled from $\pi$. \\

\noindent \textbf{Off-policy learning}: Learn over someone's shoulder, by learning about policy
$\pi$ from experience sampled from $\mu$.

\subsection{On-Policy Monte-Carlo Control}

Policy evaluation is done using $Q(s, a)$ since it is model free (i.e. does not
depend on $\mathcal{P}$ and $\mathcal{R}$). \\

\noindent $\epsilon$-Greedy Exploration:

\begin{equation*}
    \pi(a|s) = \begin{cases}
        \epsilon / m + 1 - \epsilon & \text{if } a^* = \argmax_{a \in \mathcal{A}} Q(s, a) \\
        \epsilon / m                & \text{otherwise}
    \end{cases}
\end{equation*}

\noindent It can be proof that for any $\epsilon$-greedy policy $\pi$, the
$\epsilon$-greedy policy $\pi^\prime$ with respect to $Q_\pi$ is an
improvement, i.e. $v_{\pi\prime}(s) \ge v_\pi(s)$. \\

\noindent Greedy in the Limit with Infinite Exploration (GLIE) is a set of two conditions
that, if met, ensure that an algorithm will eventually find the optimal policy.
The conditions are:

\begin{itemize}
    \item \textbf{Infinite Exploration}: All state-action pairs are explored infinitely many times,
          $\lim_{k \to \infty} N_k(s, a) = \infty$ where $N_k(s, a)$ is the number of times action
          $a$ has been taken in state $s$ up to time step $k$.
    \item \textbf{Greedy in the Limit}: The policy converges to a greedy policy,
          $\lim_{k \to \infty} \pi_k(a|s) = 1$ if $a = \argmax_{a \in \mathcal{A}} Q_k(s, a)$ and
          0 otherwise.
\end{itemize}

\noindent The GLIE Monte-Carlo control algorithm is as follows:

\begin{enumerate}
    \item Sample $k$th episode using $\pi$: $\{S_1, A_1, R_2, \dots, S_T\} \sim \pi$
    \item For each state $S_t$ and action $A_t$ in the episode,
          \begin{itemize}
              \item $N(S_t, A_t) \leftarrow N(S_t, A_t) + 1$
              \item $Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \frac{1}{N(S_t, A_t)} (G_t - Q(S_t, A_t))$
          \end{itemize}
    \item Improve policy based on the new action-value function
          \begin{itemize}
              \item $\epsilon \leftarrow 1/k$
              \item $\pi \leftarrow \epsilon\text{-greedy}(Q)$
          \end{itemize}
\end{enumerate}

\noindent GLIE Monte-Carlo control converges to the optimal action-value function, $Q(s,
    a) \rightarrow q_*(s, a)$

\subsection{On-Policy Temporal-Difference Control}

\textbf{Sarsa} algorithm:

\begin{itemize}
    \item Policy evaluation: $Q(S, A) \leftarrow Q(S, A) + \alpha (R + \gamma Q(S', A') -
              Q(S, A))$
    \item Policy improvement: $\epsilon$-greedy policy improvement is used
\end{itemize}

\noindent Sarsa algorithm for on-policy control is as follows:

\begin{itemize}
    \item Initialize $Q(s, a), \forall s \in \mathcal{S}, a \in \mathcal{A}(s)$,
          arbitrarily, and $Q(\text{terminal-state}, \cdot) = 0$
    \item Repeat (for each episode):
          \begin{itemize}
              \item Initialize $S$
              \item Choose $A$ from $S$ using policy derived from $Q$ (e.g., $\epsilon$-greedy)
              \item Repeat (for each step of episode):
                    \begin{itemize}
                        \item Take action $A$, observe $R, S'$
                        \item Choose $A'$ from $S'$ using policy derived from $Q$ (e.g., $\epsilon$-greedy)
                        \item $Q(S, A) \leftarrow Q(S, A) + \alpha[R + \gamma Q(S', A') - Q(S, A)]$
                        \item $S \leftarrow S'; A \leftarrow A';$
                    \end{itemize}
              \item until $S$ is terminal
          \end{itemize}
\end{itemize}

\noindent \textbf{n-Step Sarsa}: Instead of updating $Q$ based on a single step, we can update it using n steps:

\[
    q_t^{(n)} = R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1} R_{t+n} + \gamma^n Q(S_{t+n}, A_{t+n})
\]

\noindent The n-step Sarsa policy evaluation updates is as follows:

\[
    Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left( q_t^{(n)} - Q(S_t, A_t) \right)
\]

\noindent \textbf{Forward View Sarsa($\lambda$)}: Similar to TD($\lambda$), we can define the
n-step return using weighted sum of n-step returns:

\begin{gather*}
    q_t^{\lambda} = (1 - \lambda) \sum_{n=1}^{\infty} \lambda^{n-1} q_t^{(n)} \\
    Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left( q_t^{\lambda} - Q(S_t, A_t) \right)
\end{gather*}

\noindent \textbf{Backward view Sarsa($\lambda$)} uses eligibility traces to update the
action-value function:

\begin{align*}
    E_0(s, a) & = 0                                                  \\
    E_t(s, a) & = \gamma \lambda E_{t-1}(s, a) + 1(S_t = s, A_t = a) \\
    \delta_t  & = R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \\
    Q(s, a)   & \leftarrow Q(s, a) + \alpha \delta_t E_t(s, a)
\end{align*}\

\noindent Sarsa($\lambda$) algorithm is as follows:

\begin{itemize}
    \item Initialize $Q(s, a)$ arbitrarily, for all $s \in \mathcal{S}, a \in
              \mathcal{A}(s)$
    \item Repeat (for each episode):
          \begin{itemize}
              \item $E(s, a) = 0$, for all $s \in \mathcal{S}, a \in \mathcal{A}(s)$
              \item Initialize $S, A$
              \item Repeat (for each step of episode):
                    \begin{itemize}
                        \item Take action $A$, observe $R, S'$
                        \item Choose $A'$ from $S'$ using policy derived from $Q$ (e.g., $\epsilon$-greedy)
                        \item $\delta \leftarrow R + \gamma Q(S', A') - Q(S, A)$
                        \item $E(S, A) \leftarrow E(S, A) + 1$
                        \item For all $s \in \mathcal{S}, a \in \mathcal{A}(s)$:
                              \begin{itemize}
                                  \item $Q(s, a) \leftarrow Q(s, a) + \alpha \delta E(s, a)$
                                  \item $E(s, a) \leftarrow \gamma \lambda E(s, a)$
                              \end{itemize}
                        \item $S \leftarrow S'; A \leftarrow A'$
                    \end{itemize}
              \item until $S$ is terminal
          \end{itemize}
\end{itemize}

\subsection{Off-Policy Learning}

TODO

\subsection{Summary}

TODO
