\section{Lecture 5: Model-Free Control}

Optimize the value function $v_*$ and policy $\pi_*$ of an unknown Markov
Decision Process $\langle \mathcal{S}, \mathcal{A}, \gamma, \pi \rangle$, i.e.
no knowledge of transition probability $\mathcal{P}_{ss\prime}^a$ and reward
$\mathcal{R}_s^a$, but the agent do know the actual reward and the actual state
that they end up in.\\

\noindent \textbf{On-policy learning}: Learn on the job, by learning about policy $\pi$ from
experience sampled from $\pi$. \\

\noindent \textbf{Off-policy learning}: Learn over someone's shoulder, by learning about policy
$\pi$ from experience sampled from $\mu$.

\subsection{On-Policy Monte-Carlo Control}

TODO

\subsection{On-Policy Temporal-Difference Control}

TODO

\subsection{Off-Policy Learning}

TODO

\subsection{Summary}

TODO
