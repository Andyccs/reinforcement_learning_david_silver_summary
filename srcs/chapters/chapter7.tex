\section{Policy Gradients}

In this chapter, the policy $\pi$ is parameterized (instead of using
$\epsilon$-greedy policy):

\[
    \pi_\theta(s,a) = \mathbb{P}[a|s,\theta]
\]

\noindent The objective is to find the optimal parameter $\theta$ that maximizes a
performance measure $J(\theta)$. There are three common objective functions:
\begin{itemize}
    \item \textbf{Start value:} The mean value of the start state (for episodic tasks).
          \[
              J_1(\theta) = V^{\pi_\theta}(s_1) = \mathbb{E}_{\pi_\theta}[v_1]
          \]
    \item \textbf{Average value:} The mean value over all states, weighted by the stationary distribution of the policy.
          \[
              J_{avV}(\theta) = \sum_{s} d^{\pi_\theta}(s) V^{\pi_\theta}(s)
          \]
    \item \textbf{Average reward:} The mean reward per time-step, also weighted by the stationary distribution.
          \[
              J_{avR}(\theta) = \sum_{s} d^{\pi_\theta}(s) \sum_{a} \pi_\theta(s, a) \mathcal{R}_s^a
          \]
\end{itemize}

\noindent where $d^{\pi_\theta}(s)$ is the stationary distribution of the Markov chain
for policy $\pi_\theta$.\\

\noindent Computing policy gradients by finite differentces: We don't know the true
function for $J(\theta)$, so we can't differentiate the function directly.
Instead, we can estimate the gradient by finite differences:

\[
    \frac{\partial J(\theta)}{\partial \theta_k} \approx \frac{J(\theta + \epsilon u_k) - J(\theta)}{\epsilon}
\]
where $u_k$ is a unit vector with 1 in the k-th component and 0 elsewhere.

\subsection{Monte-Carlo Policy Gradient}

The likelihood ratios:

\begin{align*}
    \nabla_\theta \pi_\theta(s,a) & = \pi_\theta(s,a) \frac{\nabla_\theta \pi_\theta(s,a)}{\pi_\theta(s,a)} \\
    \nabla_\theta \pi_\theta(s,a) & = \pi_\theta(s,a) \nabla_\theta \log \pi_\theta(s,a)                    \\
                                  & \nabla_\theta \log \pi_\theta(s,a) \text{ is called the score function}
\end{align*}

\noindent \textbf{One-Step MDP}: Consider a simple class of one-step MDPs starting in state $s
    \sim d(s)$ and receiving reward $r = \mathcal{R}_s^a$ after one time-step.

\begin{align*}
    J(\theta)               & = \mathbb{E}_{\pi_\theta}[r]                                                                              \\
                            & = \sum_{s \in S} d(s) \sum_{a \in A} \pi_\theta(s,a) \mathcal{R}_{s,a}                                    \\
    \nabla_\theta J(\theta) & = \sum_{s \in S} d(s) \sum_{a \in A} \pi_\theta(s,a) \nabla_\theta \log \pi_\theta(s,a) \mathcal{R}_{s,a} \\
                            & = \mathbb{E}_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s,a) r]
\end{align*}
notice that the $\nabla_\theta J(\theta)$ is no longer depends on $d(s)$ and we can compute the gradient by sampling without using finite differences.\\

\noindent \textbf{Policy Gradient Theorem}: The policy gradient theorem generalized the likelihood
ratio approach to multi-step MDP, by replacing instantaneous reward $r$ with
long-term value $Q^\pi(s,a)$.

\[
    \nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s,a) Q^{\pi_\theta}(s,a)]
\]

\begin{algorithm}[H]
    \caption{REINFORCE}
    \begin{algorithmic}
        \State Initialize $\theta$ arbitrarily
        \For{each episode $\{s_1, a_1, r_2, \ldots, s_{T-1}, a_{T-1}, r_T\} \sim \pi_\theta$}
        \For{$t=1$ to $T-1$}
        \State $\theta \leftarrow \theta + \alpha \nabla_\theta \log \pi_\theta(s_t, a_t) v_t$
        \EndFor
        \EndFor
        \State \textbf{return} $\theta$
    \end{algorithmic}
\end{algorithm}

\subsection{Actor-Critic Policy Gradient}

Actor-Critic methods learn value function and policy simultaneously. Critic
updates action-value function $Q_w(s,a)$ with parameter $w$ to estimate
$Q^{\pi_\theta}(s,a)$, while Actor updates policy $\pi_\theta$ with parameter
$\theta$ in the direction suggested by the critic.

\begin{align*}
    \nabla_\theta J(\theta) & \approx \mathbb{E}_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s, a) Q_w(s, a)] \\
    \Delta \theta           & = \alpha \nabla_\theta \log \pi_\theta(s, a) Q_w(s, a)
\end{align*}

\noindent All previously discussed value function approximation methods can be used in
Critic, for example, the following algorithm uses TD(0) to update the parameter
$w$ of a linear value function approximation $Q_w(s,a) = \phi(s,a)^T w$:

\begin{algorithm}[H]
    \caption{Action-Value Actor-Critic}
    \begin{algorithmic}
        \State Initialize $s, \theta, w$
        \State Sample $a \sim \pi_\theta(s,a)$
        \For{each step}
        \State Sample reward $r = \mathcal{R}_s^a$, sample transition $s' \sim \mathcal{P}_{s, \cdot}^a$
        \State Sample action $a' \sim \pi_\theta(s', a')$
        \State $\delta = r + \gamma Q_w(s', a') - Q_w(s, a)$
        \State $\theta \leftarrow \theta + \alpha \nabla_\theta \log \pi_\theta(s, a) Q_w(s, a)$
        \State $w \leftarrow w + \beta \delta \phi(s, a)$
        \State $a \leftarrow a', s \leftarrow s'$
        \EndFor
    \end{algorithmic}
\end{algorithm}

\noindent However, approximating policy gradient $\nabla_\theta J(\theta)$ introduces
bias and it might not find the right solution. The \textbf{Compatible Function
    Approximation Theorem} proofs that under certain conditions, the policy
gradient can be exact (instead of an approximation of policy gradient)!

\begin{enumerate}
    \item \textbf{Compatibility}: The value function approximator $Q_w(s,a)$ must be
          compatible with the policy $\pi_\theta(s,a)$. This is achieved if the gradient
          of the approximator with respect to its weights $w$ is equal to the score
          function of the policy.
          \[
              \nabla_w Q_w(s,a) = \nabla_\theta \log \pi_\theta(s,a)
          \]
          In practice, this is satisfied by choosing a linear approximator for the
          critic, $Q_w(s,a) = \phi(s,a)^T w$, and setting the features $\phi(s,a)$ to be
          the score function, i.e., $\phi(s,a) := \nabla_\theta \log \pi_\theta(s,a)$.

    \item \textbf{Minimum Mean-Squared Error}: The parameters $w$ of the value
          function must be chosen to minimize the mean-squared error between the
          approximated value and the true action-value function.
          \[
              \epsilon = \mathbb{E}_{\pi_\theta}[(Q^{\pi_\theta}(s,a) - Q_w(s,a))^2]
          \]
          This condition is fulfilled by training the critic using a policy evaluation
          method like TD learning, which minimizes this error over time.
\end{enumerate}

\noindent If these conditions hold, then the policy gradient is \textbf{exact} (and not an approximation):
\[
    \nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s,a) Q_w(s,a)]
\]
