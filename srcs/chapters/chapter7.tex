\section{Policy Gradients}

In this chapter, the policy $\pi$ is parameterized (instead of using
$\epsilon$-greedy policy):

\[
    \pi_\theta(s,a) = \mathbb{P}[a|s,\theta]
\]

\noindent The objective is to find the optimal parameter $\theta$ that maximizes a
performance measure $J(\theta)$. There are three common objective functions:
\begin{itemize}
    \item \textbf{Start value:} The mean value of the start state (for episodic tasks).
          \[
              J_1(\theta) = V^{\pi_\theta}(s_1) = \mathbb{E}_{\pi_\theta}[v_1]
          \]
    \item \textbf{Average value:} The mean value over all states, weighted by the stationary distribution of the policy.
          \[
              J_{avV}(\theta) = \sum_{s} d^{\pi_\theta}(s) V^{\pi_\theta}(s)
          \]
    \item \textbf{Average reward:} The mean reward per time-step, also weighted by the stationary distribution.
          \[
              J_{avR}(\theta) = \sum_{s} d^{\pi_\theta}(s) \sum_{a} \pi_\theta(s, a) \mathcal{R}_s^a
          \]
\end{itemize}

\noindent where $d^{\pi_\theta}(s)$ is the stationary distribution of the Markov chain
for policy $\pi_\theta$.\\

\noindent Computing policy gradients by finite differentces: We don't know the true
function for $J(\theta)$, so we can't differentiate the function directly.
Instead, we can estimate the gradient by finite differences:

\[
    \frac{\partial J(\theta)}{\partial \theta_k} \approx \frac{J(\theta + \epsilon u_k) - J(\theta)}{\epsilon}
\]
where $u_k$ is a unit vector with 1 in the k-th component and 0 elsewhere.

\subsection{Monte-Carlo Policy Gradient}

The likelihood ratios:

\begin{align*}
    \nabla_\theta \pi_\theta(s,a) & = \pi_\theta(s,a) \frac{\nabla_\theta \pi_\theta(s,a)}{\pi_\theta(s,a)} \\
    \nabla_\theta \pi_\theta(s,a) & = \pi_\theta(s,a) \nabla_\theta \log \pi_\theta(s,a)                    \\
                                  & \nabla_\theta \log \pi_\theta(s,a) \text{ is called the score function}
\end{align*}

\noindent \textbf{One-Step MDP}: Consider a simple class of one-step MDPs starting in state $s
    \sim d(s)$ and receiving reward $r = \mathcal{R}_s^a$ after one time-step.

\begin{align*}
    J(\theta)               & = \mathbb{E}_{\pi_\theta}[r]                                                                              \\
                            & = \sum_{s \in S} d(s) \sum_{a \in A} \pi_\theta(s,a) \mathcal{R}_{s,a}                                    \\
    \nabla_\theta J(\theta) & = \sum_{s \in S} d(s) \sum_{a \in A} \pi_\theta(s,a) \nabla_\theta \log \pi_\theta(s,a) \mathcal{R}_{s,a} \\
                            & = \mathbb{E}_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s,a) r]
\end{align*}
notice that the $\nabla_\theta J(\theta)$ is no longer depends on $d(s)$ and we can compute the gradient by sampling without using finite differences.\\

\noindent \textbf{Policy Gradient Theorem}: The policy gradient theorem generalized the likelihood
ratio approach to multi-step MDP, by replacing instantaneous reward $r$ with
long-term value $Q^\pi(s,a)$.

\[
    \nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s,a) Q^{\pi_\theta}(s,a)]
\]

\begin{algorithm}[H]
    \caption{REINFORCE}
    \begin{algorithmic}
        \State Initialize $\theta$ arbitrarily
        \For{each episode $\{s_1, a_1, r_2, \ldots, s_{T-1}, a_{T-1}, r_T\} \sim \pi_\theta$}
        \For{$t=1$ to $T-1$}
        \State $\theta \leftarrow \theta + \alpha \nabla_\theta \log \pi_\theta(s_t, a_t) v_t$
        \EndFor
        \EndFor
        \State \textbf{return} $\theta$
    \end{algorithmic}
\end{algorithm}

\subsection{Actor-Critic Policy Gradient}

Actor-Critic methods learn value function and policy simultaneously. Critic
updates action-value function $Q_w(s,a)$ with parameter $w$ to estimate
$Q^{\pi_\theta}(s,a)$, while Actor updates policy $\pi_\theta$ with parameter
$\theta$ in the direction suggested by the critic.

\begin{align*}
    \nabla_\theta J(\theta) & \approx \mathbb{E}_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s, a) Q_w(s, a)] \\
    \Delta \theta           & = \alpha \nabla_\theta \log \pi_\theta(s, a) Q_w(s, a)
\end{align*}

\noindent All previously discussed value function approximation methods can be used in
Critic, for example, the following algorithm uses TD(0) to update the parameter
$w$ of a linear value function approximation $Q_w(s,a) = \phi(s,a)^T w$:

\begin{algorithm}[H]
    \caption{Action-Value Actor-Critic}
    \begin{algorithmic}
        \State Initialize $s, \theta, w$
        \State Sample $a \sim \pi_\theta(s,a)$
        \For{each step}
        \State Sample reward $r = \mathcal{R}_s^a$, sample transition $s' \sim \mathcal{P}_{s, \cdot}^a$
        \State Sample action $a' \sim \pi_\theta(s', a')$
        \State $\delta = r + \gamma Q_w(s', a') - Q_w(s, a)$
        \State $\theta \leftarrow \theta + \alpha \nabla_\theta \log \pi_\theta(s, a) Q_w(s, a)$
        \State $w \leftarrow w + \beta \delta \phi(s, a)$
        \State $a \leftarrow a', s \leftarrow s'$
        \EndFor
    \end{algorithmic}
\end{algorithm}

\noindent However, approximating policy gradient $\nabla_\theta J(\theta)$ introduces
bias and it might not find the right solution. The \textbf{Compatible Function
    Approximation Theorem} proofs that under certain conditions, the policy
gradient can be exact (instead of an approximation of policy gradient)!

\begin{enumerate}
    \item \textbf{Compatibility}: The value function approximator $Q_w(s,a)$ must be
          compatible with the policy $\pi_\theta(s,a)$. This is achieved if the gradient
          of the approximator with respect to its weights $w$ is equal to the score
          function of the policy.
          \[
              \nabla_w Q_w(s,a) = \nabla_\theta \log \pi_\theta(s,a)
          \]
          In practice, this is satisfied by choosing a linear approximator for the
          critic, $Q_w(s,a) = \phi(s,a)^T w$, and setting the features $\phi(s,a)$ to be
          the score function, i.e., $\phi(s,a) := \nabla_\theta \log \pi_\theta(s,a)$.

    \item \textbf{Minimum Mean-Squared Error}: The parameters $w$ of the value
          function must be chosen to minimize the mean-squared error between the
          approximated value and the true action-value function.
          \[
              \epsilon = \mathbb{E}_{\pi_\theta}[(Q^{\pi_\theta}(s,a) - Q_w(s,a))^2]
          \]
          This condition is fulfilled by training the critic using a policy evaluation
          method like TD learning, which minimizes this error over time.
\end{enumerate}

\noindent If these conditions hold, then the policy gradient is \textbf{exact} (and not an approximation):
\[
    \nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s,a) Q_w(s,a)]
\]

\noindent \textbf{Baseline function and Advantage function}: To reduce the variance of policy
gradient estimates, a baseline function $B(s)$ can be subtracted from the
action-value function without introducing bias:

\begin{align*}
    \nabla_\theta J(\theta) & = \mathbb{E}_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s,a) Q^{\pi_\theta}(s,a)] - \mathbb{E}_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s,a) B(s)]    \\
                            & = \mathbb{E}_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s,a) (Q^{\pi_\theta}(s,a) - B(s))]                                                              \\
                            & = \mathbb{E}_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s,a) (Q^{\pi_\theta}(s,a) - V^{\pi_\theta}(s))] \text{, when } B(s) =  V^{\pi_\theta}(s)        \\
                            & = \mathbb{E}_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s,a) A^{\pi_\theta}(s,a)] \text{, where } A^{\pi_\theta}(s,a) \text{ is the advantage function}
\end{align*}

\noindent The term $\mathbb{E}_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s,a) B(s)]$ is equal to zero:

\begin{align*}
    \mathbb{E}_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s,a) B(s)] & = \sum_{s \in \mathcal{S}} d^{\pi_\theta}(s) \sum_{a \in \mathcal{A }} \nabla_\theta \pi_\theta(s, a) B(s) \\
                                                                     & = \sum_{s \in \mathcal{S}} d^{\pi_\theta}(s) B(s) \nabla_\theta \sum_{a \in \mathcal{A}} \pi_\theta(s, a)  \\
                                                                     & = 0 \text{, since } \nabla_\theta \sum_{a \in \mathcal{A}} \pi_\theta(s, a) = \nabla_\theta 1 = 0
\end{align*}

\noindent In order to estimate the advantage function $A^{\pi_\theta}(s,a)$, we can use
two function approximators, but this approaches has two set of parameters and
it is more complex.

\begin{align*}
    V_v(s)              & \approx V^{\pi_\theta}(s) \text{, with parameters } v                    \\
    Q_w(s,a)            & \approx Q^{\pi_\theta}(s,a) \text{, with parameters } w                  \\
    A^{\pi_\theta}(s,a) & \approx A(s,a) = Q_w(s,a) - V_v(s) \text{, combining both approximators}
\end{align*}

\noindent Alternatively, we can estimate the advantage function using temporal-difference
errors $\delta^{\pi_\theta}$, which only requires one set of critic parameters $v$:

\begin{align*}
    \delta^{\pi_\theta}                               & = r + \gamma V^{\pi_\theta}(s') - V^{\pi_\theta}(s)                                                               \\
                                                      & \approx \delta_v = r + \gamma V_v(s') - V_v(s)                                                                    \\
    \mathbb{E}_{\pi_\theta}[\delta^{\pi_\theta}|s, a] & = \mathbb{E}_{\pi_\theta}[r + \gamma V^{\pi_\theta}(s')|s, a] - V^{\pi_\theta}(s)                                 \\
                                                      & = Q^{\pi_\theta}(s, a) - V^{\pi_\theta}(s)                                                                        \\
                                                      & = A^{\pi_\theta}(s, a)                                                                                            \\
    \nabla_\theta J(\theta)                           & = \mathbb{E}_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s, a) \mathbb{E}_{\pi_\theta}[\delta^{\pi_\theta}|s, a] ] \\
                                                      & = \mathbb{E}_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s, a) \delta^{\pi_\theta}]
\end{align*}

\noindent The policy gradient can also be estimated at many time-scales. For
example, Monte-Carlo policy gradient uses error from complete return, while
actor-critic policy gradient uses the one-step TD error.

\begin{align*}
    \nabla_\theta J(\theta) & = \mathbb{E}_{{\pi_\theta}}[\nabla_\theta \log \pi_\theta(s, a) A^\pi_\theta(s, a)]                               \\
    \Delta \theta           & = \alpha(v_t - V_v(s_t))\nabla_\theta \log \pi_\theta(s_t, a_t) \text{, by Monte-Carlo policy gradient}           \\
    \Delta \theta           & = \alpha(r + \gamma V_v(s_{t+1}) - V_v(s_t))\nabla_\theta \log \pi_\theta(s_t, a_t) \text{, by one-step TD error} \\
    \Delta \theta           & = \alpha(v_t^\lambda - V_v(s_t))\nabla_\theta \log \pi_\theta(s_t, a_t) \text{, by forward-view TD($\lambda$)}    \\
    \\
    \delta                  & = r_{t+1} + \gamma V_v(s_{t+1}) - V_v(s_t)                                                                        \\
    e_{t+1}                 & = \lambda e_t + \nabla_\theta \log \pi_\theta(s_t, a_t)                                                           \\
    \Delta \theta           & = \alpha \delta_t e_t \text{, by backward-view TD($\lambda$)}
\end{align*}

